# -*- coding: utf-8 -*-
"""train_gnn_graphsage.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mq3_WaDona8fBLf8j-oHV_OU4a08Wfyj
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import SAGEConv
from torch_geometric.utils import negative_sampling

data_obj = torch.load("gnn_graph_data.pt")
edge_index = data_obj["edge_index"]
num_users = data_obj["num_users"]
num_acts  = data_obj["num_acts"]
num_nodes = num_users + num_acts

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
edge_index = edge_index.to(device)

# 1) ëª¨ë¸ ì •ì˜ (homogeneous ê·¸ë˜í”„, user+activity í•©ì³ì„œ ì‚¬ìš©)
class GraphSAGERecommender(nn.Module):
    def __init__(self, num_nodes, hidden_dim=64, num_layers=2):
        super().__init__()
        self.emb = nn.Embedding(num_nodes, hidden_dim)
        self.convs = nn.ModuleList()
        for _ in range(num_layers):
            self.convs.append(SAGEConv(hidden_dim, hidden_dim))

    def forward(self, edge_index):
        x = self.emb.weight
        for conv in self.convs:
            x = conv(x, edge_index)
            x = F.relu(x)
        return x  # [num_nodes, hidden_dim]

model = GraphSAGERecommender(num_nodes=num_nodes, hidden_dim=64, num_layers=2).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# 2) ê°„ë‹¨í•œ ë§í¬ì˜ˆì¸¡ í•™ìŠµ (positive edges vs negative edges)
def train(num_epochs=50):
    model.train()
    for epoch in range(1, num_epochs + 1):
        optimizer.zero_grad()

        z = model(edge_index)  # ë…¸ë“œ ì„ë² ë”©

        # positive edge score
        src, dst = edge_index
        pos_score = (z[src] * z[dst]).sum(dim=1)  # dot product

        # negative samples
        neg_edge_index = negative_sampling(
            edge_index=edge_index,
            num_nodes=num_nodes,
            num_neg_samples=src.size(0),
        )
        nsrc, ndst = neg_edge_index
        neg_score = (z[nsrc] * z[ndst]).sum(dim=1)

        # loss: sigmoid BCE
        loss_pos = F.binary_cross_entropy_with_logits(pos_score, torch.ones_like(pos_score))
        loss_neg = F.binary_cross_entropy_with_logits(neg_score, torch.zeros_like(neg_score))
        loss = loss_pos + loss_neg

        loss.backward()
        optimizer.step()

        if epoch % 10 == 0:
            print(f"Epoch {epoch:03d} | loss = {loss.item():.4f}")

train(num_epochs=80)

# 3) ìµœì¢… ì„ë² ë”© ì¶”ì¶œ (í™œë™ë§Œ)
model.eval()
with torch.no_grad():
    z = model(edge_index)  # [num_nodes, dim]

act_emb = z[num_users : num_users + num_acts].cpu()  # í™œë™ ë¶€ë¶„ë§Œ
torch.save(
    {
        "activity_embeddings": act_emb,
        "act2idx": data_obj["act2idx"],
    },
    "activity_gnn_embeddings.pt",
)
print("ğŸ¯ activity_gnn_embeddings.pt ì €ì¥ ì™„ë£Œ!")